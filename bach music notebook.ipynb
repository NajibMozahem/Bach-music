{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of this code was adopted from the solutions to the excercies for the book 'Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. The goal of this notebook is to walk through a possible solution to the excercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project requires us to download the Bach chorales dataset and then to train a model that would predict the next note given an input sequence. We start by downloading the data set and unzipping the files.\n",
    "\n",
    "Once the files are downloaded they are read as pandas using the read_csv() method. Notice that there are different files, so this means that the load_chorales() function is going to return a list of the contents of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/\"\n",
    "FILENAME = \"jsb_chorales.tgz\"\n",
    "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, cache_subdir='datasets/jsb_chorales', extract=True)\n",
    "jsb_chorales_dir = Path(filepath).parent\n",
    "train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n",
    "valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n",
    "test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))\n",
    "\n",
    "def load_chorales(filepaths):\n",
    "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
    "\n",
    "train_chorales = load_chorales(train_files)\n",
    "valid_chorales = load_chorales(valid_files)\n",
    "test_chorales = load_chorales(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the first element in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[74, 70, 65, 58],\n",
       " [74, 70, 65, 58],\n",
       " [74, 70, 65, 58],\n",
       " [74, 70, 65, 58],\n",
       " [75, 70, 58, 55],\n",
       " [75, 70, 58, 55],\n",
       " [75, 70, 60, 55],\n",
       " [75, 70, 60, 55],\n",
       " [77, 69, 62, 50],\n",
       " [77, 69, 62, 50],\n",
       " [77, 69, 62, 50],\n",
       " [77, 69, 62, 50],\n",
       " [77, 70, 62, 55],\n",
       " [77, 70, 62, 55],\n",
       " [77, 69, 62, 55],\n",
       " [77, 69, 62, 55],\n",
       " [75, 67, 63, 48],\n",
       " [75, 67, 63, 48],\n",
       " [75, 69, 63, 48],\n",
       " [75, 69, 63, 48],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [75, 69, 63, 48],\n",
       " [75, 69, 63, 48],\n",
       " [75, 67, 63, 48],\n",
       " [75, 67, 63, 48],\n",
       " [77, 65, 62, 50],\n",
       " [77, 65, 62, 50],\n",
       " [77, 65, 60, 50],\n",
       " [77, 65, 60, 50],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 53],\n",
       " [74, 67, 58, 53],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [74, 71, 53, 50],\n",
       " [74, 71, 53, 50],\n",
       " [74, 71, 53, 50],\n",
       " [74, 71, 53, 50],\n",
       " [75, 72, 55, 48],\n",
       " [75, 72, 55, 48],\n",
       " [75, 72, 55, 50],\n",
       " [75, 72, 55, 50],\n",
       " [75, 67, 60, 51],\n",
       " [75, 67, 60, 51],\n",
       " [75, 67, 60, 53],\n",
       " [75, 67, 60, 53],\n",
       " [74, 67, 60, 55],\n",
       " [74, 67, 60, 55],\n",
       " [74, 67, 57, 55],\n",
       " [74, 67, 57, 55],\n",
       " [74, 65, 59, 43],\n",
       " [74, 65, 59, 43],\n",
       " [72, 63, 59, 43],\n",
       " [72, 63, 59, 43],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [75, 67, 60, 60],\n",
       " [75, 67, 60, 60],\n",
       " [75, 67, 60, 60],\n",
       " [75, 67, 60, 60],\n",
       " [77, 70, 62, 58],\n",
       " [77, 70, 62, 58],\n",
       " [77, 70, 62, 56],\n",
       " [77, 70, 62, 56],\n",
       " [79, 70, 62, 55],\n",
       " [79, 70, 62, 55],\n",
       " [79, 70, 62, 53],\n",
       " [79, 70, 62, 53],\n",
       " [79, 70, 63, 51],\n",
       " [79, 70, 63, 51],\n",
       " [79, 70, 63, 51],\n",
       " [79, 70, 63, 51],\n",
       " [77, 70, 63, 58],\n",
       " [77, 70, 63, 58],\n",
       " [77, 70, 60, 58],\n",
       " [77, 70, 60, 58],\n",
       " [77, 70, 62, 46],\n",
       " [77, 70, 62, 46],\n",
       " [77, 68, 62, 46],\n",
       " [75, 68, 62, 46],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [75, 67, 58, 53],\n",
       " [75, 67, 58, 53],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [77, 65, 58, 50],\n",
       " [77, 65, 58, 50],\n",
       " [77, 65, 56, 50],\n",
       " [77, 65, 56, 50],\n",
       " [70, 63, 55, 51],\n",
       " [70, 63, 55, 51],\n",
       " [70, 63, 55, 51],\n",
       " [70, 63, 55, 51],\n",
       " [75, 65, 60, 45],\n",
       " [75, 65, 60, 45],\n",
       " [75, 65, 60, 45],\n",
       " [75, 65, 60, 45],\n",
       " [74, 65, 58, 46],\n",
       " [74, 65, 58, 46],\n",
       " [74, 65, 58, 46],\n",
       " [74, 65, 58, 46],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [75, 67, 58, 57],\n",
       " [75, 67, 58, 57],\n",
       " [75, 67, 58, 55],\n",
       " [75, 67, 58, 55],\n",
       " [77, 65, 60, 57],\n",
       " [77, 65, 60, 57],\n",
       " [77, 65, 60, 53],\n",
       " [77, 65, 60, 53],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chorales[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first list is actually a list of lists. We see that the inner most list is a list of four integers. These integers represent to a note's index. Ultimately, the algorithm must be able to predict the notes, given a certain input. \n",
    "\n",
    "The above output shows just the first list in the training set. How many lists are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_chorales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is a total of 229 lists (each corresponds to a chorales which is composed by Bach). In each of these lists (or chorales) there is another list of a list of four integers. In other words, we have three nested lists.\n",
    "\n",
    "Before we go further, let us calculate the number of unique notes, the lowest note and the highest note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = set()\n",
    "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord)\n",
    "number_of_notes = len(notes)\n",
    "min_note = min(notes - {0})\n",
    "max_note = max(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to visualise the data at every step of data preparing because it helps me understand what is happening. The training set is too big to be visualized, so I will create a small sample that includes only the first two chorales, and from each chorales I will only take the first six lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[74, 70, 65, 58],\n",
       "  [74, 70, 65, 58],\n",
       "  [74, 70, 65, 58],\n",
       "  [74, 70, 65, 58],\n",
       "  [75, 70, 58, 55],\n",
       "  [75, 70, 58, 55]],\n",
       " [[69, 64, 61, 57],\n",
       "  [69, 64, 61, 57],\n",
       "  [69, 64, 61, 57],\n",
       "  [69, 64, 61, 57],\n",
       "  [71, 64, 59, 56],\n",
       "  [71, 64, 59, 56]]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_two_lists = train_chorales[0:2]\n",
    "first_two_lists_subset = [lst[0:6] for lst in first_two_lists]\n",
    "first_two_lists_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more manageable data set for us for visualization. We here have three nested lists. The two outer lists represent two chorales. Inside each of these two lists, we have six lists (which are the first six lists of the original training set).\n",
    "\n",
    "So what do we want to do? Since we are dealing with sequences, we woudl like to train some type of RNN. We also need to prepare the training data by dividing it into an inout and output. The input is going to be some notes, and the output will be notes. \n",
    "\n",
    "We have several options here. First, we can train a model that will predict the output given a certain number of inputs. In other words, we divide the data set into windows of length l, and for each input of length l, we will have an output of four notes.\n",
    "\n",
    "However, instead of having a model that will predict the next four notes, why not have a model that will predict only the next node? So we make a prediction one note at a time. This will decrease the chance of the algorithm producing four notes at once where these four notes do not go hand in hand. Therefore, we will follow this approach. This means that we are going to treat the notes as a long series of individual notes, not as a group of four. \n",
    "\n",
    "Another decision that we have to make here is do we want to fit a sequence to vector model or a seuence to seuqnce model? In a sequence to vector approach, we train the model to predict the next note only at the very last time step. In a sequence to sequence appraoch, we train the model to predict the next note at each and every time step. As you can imagine, the sequence to sequence appraoch would give us beter results in this case.\n",
    "\n",
    "So how do we train a model using the sequence to sequence approach? What we simply need is to train the model to predict one time step into the future. In other words, the output will simply be the input shifted by one time step. To do that, we will use the tensorflow data sets. We will perform the operations on the small sample first so that we can visualise what is happenign along the way. Once we are sure that the data is being preprocessed properly, we will perform the same operations on the train, valid, and test data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = tf.ragged.constant(first_two_lists_subset, ragged_rank=1)\n",
    "sample_data = tf.data.Dataset.from_tensor_slices(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code simply converts the sample data to tensors. The first line converts the sample data to ragged tensors. Ragged tensors are tensors that are not of equal length. Since each chorales has a different length, we need to convert them first to ragged tensors. The second line then finishes this transformation. Let us look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[74 70 65 58]\n",
      " [74 70 65 58]\n",
      " [74 70 65 58]\n",
      " [74 70 65 58]\n",
      " [75 70 58 55]\n",
      " [75 70 58 55]], shape=(6, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[69 64 61 57]\n",
      " [69 64 61 57]\n",
      " [69 64 61 57]\n",
      " [69 64 61 57]\n",
      " [71 64 59 56]\n",
      " [71 64 59 56]], shape=(6, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for element in sample_data:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the two outer lists were converted to two tensors. We also want to convert the inner lists to tensors. Once we convert them to tensors, we can use the window function in order to create a window that we will use to create the input and output data sets. The command after that uses the batch function on each window since the window function creates a dataset that contains windows where each window is a dataset. This mean sthat the result is a nested data set. We therefore use the flat_map function to flatten the datasets and we apply the batch function with the same size as the window size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[74 70 65 58]\n",
      " [74 70 65 58]\n",
      " [74 70 65 58]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[74 70 65 58]\n",
      " [74 70 65 58]\n",
      " [74 70 65 58]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[74 70 65 58]\n",
      " [74 70 65 58]\n",
      " [75 70 58 55]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[74 70 65 58]\n",
      " [75 70 58 55]\n",
      " [75 70 58 55]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[69 64 61 57]\n",
      " [69 64 61 57]\n",
      " [69 64 61 57]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[69 64 61 57]\n",
      " [69 64 61 57]\n",
      " [69 64 61 57]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[69 64 61 57]\n",
      " [69 64 61 57]\n",
      " [71 64 59 56]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[69 64 61 57]\n",
      " [71 64 59 56]\n",
      " [71 64 59 56]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "shift = 1\n",
    "sample_data = sample_data.map(lambda row: tf.data.Dataset.from_tensor_slices(row).window(window_size + 1, shift=shift, drop_remainder=True))\n",
    "sample_data = sample_data.flat_map(lambda row: row.flat_map(lambda window: window.batch(window_size + 1)))\n",
    "for element in sample_data:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the output, you will see that instead of just two tensors we now have a series of them, were each is made up of three lists, because we chose a window size of 2 + 1. We also notice that as we move down, the window shifts one step at a time, since we specified that the sift parameter is equal to 1. The nice thing here is that this shiftin does not roll over from one chorales to the other. In other words, there is no window that contains notes from each chorales. At each chorales we start with a new window. This is exactly how we want it.\n",
    "\n",
    "The next step is to convert these windows to one dimensional arrays so that the input becomes simply a series of numbers. We also want to shift the values of the notes so that they start from zero. The reason I want to do that is that in the model creation section, I want plan to use an embedded layer, and we need to map the input numbers to the input dimension of that layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x0000020BD42EB7F0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x0000020BD42EB7F0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x0000020BD42EB7F0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x0000020BD42EB7F0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x0000020BD42EB880> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x0000020BD42EB880>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x0000020BD42EB880> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x0000020BD42EB880>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[39, 35, 30, 23, 39, 35, 30, 23, 39, 35, 30, 23]\n",
      "[39, 35, 30, 23, 39, 35, 30, 23, 39, 35, 30, 23]\n",
      "[39, 35, 30, 23, 39, 35, 30, 23, 40, 35, 23, 20]\n",
      "[39, 35, 30, 23, 40, 35, 23, 20, 40, 35, 23, 20]\n",
      "[34, 29, 26, 22, 34, 29, 26, 22, 34, 29, 26, 22]\n",
      "[34, 29, 26, 22, 34, 29, 26, 22, 34, 29, 26, 22]\n",
      "[34, 29, 26, 22, 34, 29, 26, 22, 36, 29, 24, 21]\n",
      "[34, 29, 26, 22, 36, 29, 24, 21, 36, 29, 24, 21]\n"
     ]
    }
   ],
   "source": [
    "sample_data = sample_data.map(lambda window: tf.where(window == 0, window, window - min_note + 1)).map(lambda window: tf.reshape(window, [-1]))\n",
    "for window in sample_data:\n",
    "    print([elem.numpy() for elem in window])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output we see that we no whave a sequence of numbers. The numbers seem different. This is because we have shifted them. Notice that each line contains 12 numbers. This is because we created windows of size three where each window had four notes. We are now ready to divide the dat ainto input and output. Remember that we are going to use the sequence to sequence appraoch. This means that the output is simply the inputed shifted by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [39 35 30 23 39 35 30 23 39 35 30] Target: [35 30 23 39 35 30 23 39 35 30 23]\n",
      "Input: [39 35 30 23 39 35 30 23 39 35 30] Target: [35 30 23 39 35 30 23 39 35 30 23]\n",
      "Input: [39 35 30 23 39 35 30 23 40 35 23] Target: [35 30 23 39 35 30 23 40 35 23 20]\n",
      "Input: [39 35 30 23 40 35 23 20 40 35 23] Target: [35 30 23 40 35 23 20 40 35 23 20]\n",
      "Input: [34 29 26 22 34 29 26 22 34 29 26] Target: [29 26 22 34 29 26 22 34 29 26 22]\n",
      "Input: [34 29 26 22 34 29 26 22 34 29 26] Target: [29 26 22 34 29 26 22 34 29 26 22]\n",
      "Input: [34 29 26 22 34 29 26 22 36 29 24] Target: [29 26 22 34 29 26 22 36 29 24 21]\n",
      "Input: [34 29 26 22 36 29 24 21 36 29 24] Target: [29 26 22 36 29 24 21 36 29 24 21]\n"
     ]
    }
   ],
   "source": [
    "sample_data = sample_data.map(lambda window: (window[:-1], window[1:]))\n",
    "for X, y in sample_data:\n",
    "    print(\"Input:\", X.numpy(), \"Target:\", y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good. We now need to process the train, valid, and test data sets using these commands. We can create a function in order to avoid repetition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_data.<locals>.<lambda> at 0x0000020BD48F0CA0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function prepare_data.<locals>.<lambda> at 0x0000020BD48F0CA0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function prepare_data.<locals>.<lambda> at 0x0000020BD48F0CA0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function prepare_data.<locals>.<lambda> at 0x0000020BD48F0CA0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_data.<locals>.<lambda> at 0x0000020BD6B15FC0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function prepare_data.<locals>.<lambda> at 0x0000020BD6B15FC0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function prepare_data.<locals>.<lambda> at 0x0000020BD6B15FC0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function prepare_data.<locals>.<lambda> at 0x0000020BD6B15FC0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_data.<locals>.<lambda> at 0x0000020BD6B15B40> and will run it as-is.\n",
      "Cause: could not parse the source code of <function prepare_data.<locals>.<lambda> at 0x0000020BD6B15B40>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function prepare_data.<locals>.<lambda> at 0x0000020BD6B15B40> and will run it as-is.\n",
      "Cause: could not parse the source code of <function prepare_data.<locals>.<lambda> at 0x0000020BD6B15B40>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_data.<locals>.<lambda> at 0x0000020BD6B16950> and will run it as-is.\n",
      "Cause: could not parse the source code of <function prepare_data.<locals>.<lambda> at 0x0000020BD6B16950>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function prepare_data.<locals>.<lambda> at 0x0000020BD6B16950> and will run it as-is.\n",
      "Cause: could not parse the source code of <function prepare_data.<locals>.<lambda> at 0x0000020BD6B16950>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_data.<locals>.<lambda> at 0x0000020BD6B14700> and will run it as-is.\n",
      "Cause: could not parse the source code of <function prepare_data.<locals>.<lambda> at 0x0000020BD6B14700>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function prepare_data.<locals>.<lambda> at 0x0000020BD6B14700> and will run it as-is.\n",
      "Cause: could not parse the source code of <function prepare_data.<locals>.<lambda> at 0x0000020BD6B14700>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_data.<locals>.<lambda> at 0x0000020BD6B164D0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function prepare_data.<locals>.<lambda> at 0x0000020BD6B164D0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function prepare_data.<locals>.<lambda> at 0x0000020BD6B164D0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function prepare_data.<locals>.<lambda> at 0x0000020BD6B164D0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda window: tf.reshape(window, [-1])\n",
      "\n",
      "Match 1:\n",
      "lambda window: tf.where(window == 0, window, window - min_note + 1)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(to_process):\n",
    "    dataset = tf.ragged.constant(to_process, ragged_rank=1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    # we use a window_size of 32 since we are now using the whole data and not just experimenting with a small subset\n",
    "    window_size = 32\n",
    "    shift = 1\n",
    "    dataset = dataset.map(lambda row: tf.data.Dataset.from_tensor_slices(row).window(window_size + 1, shift=shift, drop_remainder=True))\n",
    "    dataset = dataset.flat_map(lambda row: row.flat_map(lambda window: window.batch(window_size + 1)))\n",
    "    dataset = dataset.map(lambda window: tf.where(window == 0, window, window - min_note + 1)).map(lambda window: tf.reshape(window, [-1]))\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[1:]))\n",
    "    dataset = dataset.batch(32)\n",
    "    return dataset\n",
    "\n",
    "train_set = prepare_data(train_chorales)\n",
    "valid_set = prepare_data(valid_chorales)\n",
    "test_set = prepare_data(test_chorales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the model. We will use an embedding layer, since it makes more since to treat the note numbers as categorical data and not as numerical data. This input will be projected on 6 dimensions. We will then add two LSTM layers, each with 64 neurons. We will also set the dropout parameter to 0.2 as a form of regularization. Finally, we will use a TimeDistributed layer on the output layer in order to train the model using the sequence to sequence appraoch. The output layer will contain one node for each note. A softmax activation will be used to calculate the probabilities of each of these notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 6)           282       \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 64)          18176     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, None, 64)          33024     \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, None, 47)         3055      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,537\n",
      "Trainable params: 54,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(number_of_notes, 6, input_shape=[None]),\n",
    "    keras.layers.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed( keras.layers.Dense(number_of_notes, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1497/1497 [==============================] - 477s 315ms/step - loss: 2.9168 - accuracy: 0.1622 - val_loss: 2.2305 - val_accuracy: 0.2788\n",
      "Epoch 2/20\n",
      "1497/1497 [==============================] - 467s 312ms/step - loss: 2.0748 - accuracy: 0.3219 - val_loss: 1.9285 - val_accuracy: 0.3808\n",
      "Epoch 3/20\n",
      "1497/1497 [==============================] - 470s 314ms/step - loss: 1.8252 - accuracy: 0.4107 - val_loss: 1.6888 - val_accuracy: 0.4654\n",
      "Epoch 4/20\n",
      "1497/1497 [==============================] - 470s 314ms/step - loss: 1.6335 - accuracy: 0.4883 - val_loss: 1.5016 - val_accuracy: 0.5354\n",
      "Epoch 5/20\n",
      "1497/1497 [==============================] - 475s 317ms/step - loss: 1.4918 - accuracy: 0.5435 - val_loss: 1.3734 - val_accuracy: 0.5892\n",
      "Epoch 6/20\n",
      "1497/1497 [==============================] - 466s 311ms/step - loss: 1.3759 - accuracy: 0.5909 - val_loss: 1.2525 - val_accuracy: 0.6363\n",
      "Epoch 7/20\n",
      "1497/1497 [==============================] - 470s 314ms/step - loss: 1.2718 - accuracy: 0.6303 - val_loss: 1.1480 - val_accuracy: 0.6770\n",
      "Epoch 8/20\n",
      "1497/1497 [==============================] - 466s 311ms/step - loss: 1.1906 - accuracy: 0.6575 - val_loss: 1.0783 - val_accuracy: 0.6965\n",
      "Epoch 9/20\n",
      "1497/1497 [==============================] - 466s 311ms/step - loss: 1.1312 - accuracy: 0.6769 - val_loss: 1.0287 - val_accuracy: 0.7124\n",
      "Epoch 10/20\n",
      "1497/1497 [==============================] - 465s 311ms/step - loss: 1.0841 - accuracy: 0.6922 - val_loss: 0.9871 - val_accuracy: 0.7260\n",
      "Epoch 11/20\n",
      "1497/1497 [==============================] - 462s 308ms/step - loss: 1.0463 - accuracy: 0.7036 - val_loss: 0.9535 - val_accuracy: 0.7370\n",
      "Epoch 12/20\n",
      "1497/1497 [==============================] - 465s 310ms/step - loss: 1.0143 - accuracy: 0.7130 - val_loss: 0.9250 - val_accuracy: 0.7469\n",
      "Epoch 13/20\n",
      "1497/1497 [==============================] - 470s 314ms/step - loss: 0.9875 - accuracy: 0.7206 - val_loss: 0.9036 - val_accuracy: 0.7526\n",
      "Epoch 14/20\n",
      "1497/1497 [==============================] - 468s 313ms/step - loss: 0.9648 - accuracy: 0.7270 - val_loss: 0.8860 - val_accuracy: 0.7579\n",
      "Epoch 15/20\n",
      "1497/1497 [==============================] - 467s 312ms/step - loss: 0.9450 - accuracy: 0.7323 - val_loss: 0.8688 - val_accuracy: 0.7631\n",
      "Epoch 16/20\n",
      "1497/1497 [==============================] - 465s 310ms/step - loss: 0.9272 - accuracy: 0.7373 - val_loss: 0.8556 - val_accuracy: 0.7665\n",
      "Epoch 17/20\n",
      "1497/1497 [==============================] - 465s 311ms/step - loss: 0.9124 - accuracy: 0.7413 - val_loss: 0.8430 - val_accuracy: 0.7698\n",
      "Epoch 18/20\n",
      "1497/1497 [==============================] - 465s 310ms/step - loss: 0.8986 - accuracy: 0.7450 - val_loss: 0.8320 - val_accuracy: 0.7733\n",
      "Epoch 19/20\n",
      "1497/1497 [==============================] - 467s 312ms/step - loss: 0.8873 - accuracy: 0.7482 - val_loss: 0.8233 - val_accuracy: 0.7763\n",
      "Epoch 20/20\n",
      "1497/1497 [==============================] - 466s 312ms/step - loss: 0.8765 - accuracy: 0.7509 - val_loss: 0.8153 - val_accuracy: 0.7782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20bd8215210>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_set, epochs=20, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy on the valid_set is 77.82%. We can now use the model to find the accuracy on the thirs set which the model was not exposed to during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 23s 45ms/step - loss: 0.8496 - accuracy: 0.7608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8496134281158447, 0.7608373165130615]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the solutions to the book excerices, the author used a different model. The model he used was more complex and was made up of a series of convolution layers and normalization layers, followed by an LSTM layer. The result was a model that was much deeper than the one that we used. The author also specified a specific learning rate. Let us see what results we would get with that model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "    133/Unknown - 41s 277ms/step - loss: 2.4688 - accuracy: 0.3596"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=number_of_notes, output_dim=5,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.LSTM(256, return_sequences=True),\n",
    "    keras.layers.Dense(number_of_notes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=20, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
